{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ2wL34_s-GZ"
      },
      "outputs": [],
      "source": [
        "# Use RL to finetune a model to learn the countdown task\n",
        "# Works with Google Colab on A100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  -U -q trl peft"
      ],
      "metadata": {
        "id": "yF8-njJAvDyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "cW70fJ_MRUMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from Hugging Face Hub\n",
        "dataset_id = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
        "dataset = load_dataset(dataset_id, split=\"train\")\n",
        "# select a random subset of 50k samples\n",
        "dataset = dataset.shuffle(seed=42).select(range(50000))\n",
        "\n",
        "# Load tokenizer from Hugging Face Hub to format the dataset to our \"r1\" prompt\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "\n",
        "# gemerate r1 prompt with a prefix for the model to already start with the thinking process\n",
        "def generate_r1_prompt(numbers, target):\n",
        "    r1_prefix = [{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
        "      }]\n",
        "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), \"target\": target}\n",
        "\n",
        "# convert our dataset to the r1 prompt\n",
        "dataset = dataset.map(lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]))\n",
        "\n",
        "# split the dataset into train and test\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "test_dataset = train_test_split[\"test\"]"
      ],
      "metadata": {
        "id": "TeGXEzkAPMQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def format_reward_func(completions, target, **kwargs):\n",
        "    \"\"\"\n",
        "    Format: <think>...</think><answer>...</answer>\n",
        "    Args:\n",
        "        completions (list[str]): Generated outputs\n",
        "        target (list[str]): Expected answers\n",
        "\n",
        "      Returns:\n",
        "          list[float]: Reward scores\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for completion, gt in zip(completions, target):\n",
        "\n",
        "      try:\n",
        "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
        "        completion = \"<think>\" + completion\n",
        "        # Check if the format is correct\n",
        "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
        "\n",
        "        match = re.search(regex, completion, re.DOTALL)\n",
        "        # if the format is not correct, reward is 0\n",
        "        if match is None or len(match.groups()) != 2:\n",
        "            rewards.append(0.0)\n",
        "        else:\n",
        "            rewards.append(1.0)\n",
        "      except Exception:\n",
        "        rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "def equation_reward_func(completions, target, nums, **kwargs):\n",
        "    \"\"\"\n",
        "    Evaluates completions based on:\n",
        "    2. Mathematical correctness of the answer\n",
        "\n",
        "    Args:\n",
        "        completions (list[str]): Generated outputs\n",
        "        target (list[str]): Expected answers\n",
        "        nums (list[str]): Available numbers\n",
        "\n",
        "    Returns:\n",
        "        list[float]: Reward scores\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for completion, gt, numbers in zip(completions, target, nums):\n",
        "      try:\n",
        "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
        "        completion = \"<think>\" + completion\n",
        "        # Check if the format is correct\n",
        "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
        "        if match is None:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        # Extract the \"answer\" part from the completion\n",
        "        equation = match.group(1).strip()\n",
        "        # Extract all numbers from the equation\n",
        "        used_numbers = [int(n) for n in re.findall(r'\\d+', equation)]\n",
        "\n",
        "        # Check if all numbers are used exactly once\n",
        "        if sorted(used_numbers) != sorted(numbers):\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
        "        allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n",
        "        if not re.match(allowed_pattern, equation):\n",
        "           rewards.append(0.0)\n",
        "           continue\n",
        "\n",
        "        # Evaluate the equation with restricted globals and locals\n",
        "        result = eval(equation, {\"__builti'ns__\": None}, {})\n",
        "        # Check if the equation is correct and matches the ground truth\n",
        "        if abs(float(result) - float(gt)) < 1e-5:\n",
        "            rewards.append(1.0)\n",
        "        else:\n",
        "            rewards.append(0.0)\n",
        "      except Exception:\n",
        "            # If evaluation fails, reward is 0\n",
        "            rewards.append(0.0)\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "Q6--cgc3PlHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_sample_1 = \"\"\"We need to find an equation using the numbers 19, 36, 55, and 7\n",
        "exactly once, with basic arithmetic operations, that equals 65. One possible\n",
        "combination is 55 + 36 - 19 + 7... </think>\n",
        "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
        "\n",
        "correct_sample_2 = \"\"\" ... </think>\n",
        "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
        "\n",
        "wrong_format = \"\"\"User: Using the numbers [19, 36, 55, 7], create an equation that equals 65.\"\"\"\n",
        "\n",
        "wrong_format_2 = \"\"\"To find the equation that equals 79 using the numbers 95, 78, 6, 88, I'll start by adding 88 and 95:\n",
        "95 + 88 = 183\n",
        "Now, let's subtract 104 from 183 to get 79:\n",
        "183 - 104 = 79\n",
        "<think> 183 - 104 = 79 </think><think> 183 - 104 = 79 </think><answer> 183 - 104 = 79 </answer>\"\"\"\n",
        "\n",
        "wrong_result = \"\"\" ... </think>\n",
        "<answer> 55 + 36 - 7 - 18 </answer>\"\"\"\n",
        "\n",
        "\n",
        "test_rewards = format_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
        "assert test_rewards == [1.0, 1.0, 0.0, 0.0, 1.0], \"Reward function is not working\"\n",
        "test_rewards = equation_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
        "assert test_rewards == [1.0, 1.0, 0.0, 0.0, 0.0], \"Reward function is not working\""
      ],
      "metadata": {
        "id": "b2qHlFszPtmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
        "\n",
        "# our model we are going to use as policy\n",
        "model_config = ModelConfig(\n",
        "    model_name_or_path=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    torch_dtype=\"bfloat16\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    use_peft=True,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Hyperparameters\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"qwen-r1-aha-moment\",\n",
        "    learning_rate=5e-7,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    max_steps=100,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    bf16=True,\n",
        "    # GRPO specific parameters\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=1024, # max length of the generated output for our solution\n",
        "    num_generations=2,\n",
        "    beta=0.001,\n",
        "\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model_config.model_name_or_path,\n",
        "    reward_funcs=[format_reward_func, equation_reward_func],\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    peft_config=get_peft_config(model_config),\n",
        ")"
      ],
      "metadata": {
        "id": "pOE7T4ttP1CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and push the model to the Hub\n",
        "trainer.train()\n",
        "# Save model\n",
        "trainer.save_model(training_args.output_dir)"
      ],
      "metadata": {
        "id": "3qLC9jLrP6vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A65pPobeMi6Z"
      }
    }
  ]
}